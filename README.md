<div align="center">   
    
## üéäReconnaissance des Expressions Faciales avec SNNs et Cam√©ras √âv√©nementiellesüéä
</div>

</div align="center">  

(https://arxiv.org/pdf/2304.10211.pdf)

</div>
<!-- TABLE OF CONTENTS -->
<details>
  <summary>Table of Contents</summary>

- [üìç Introduction](#-overview)
- [üëæ Technologies Utilis√©es](#-demo)
- [üß© Installation et configuration ](#-features)
- [üóÇÔ∏è Structure du projet](#Ô∏è-examples)
- [üì¶ Utilisation de v2e pour la Conversion Vid√©o-en-√âv√©nements](#Ô∏è-configuration)
- [üöÄ Modification du code train.py](#-getting-started)
- [ü§ñ Comment Utiliser ](#-Comment-Utiliser)
- [üß™ R√©sultat et Interpr√©tation](#-roadmap)
- [üßë‚Äçüíª Contributing](#-Citation)
- [üéó License](#-license)
</details>

---

#  üìç Introduction 

Ce projet repr√©sente une √©volution d'un syst√®me pr√©existant de reconnaissance des expressions faciales (FER) en exploitant les r√©seaux de neurones √† impulsions (Spiking Neural Networks, SNNs) et les cam√©ras √©v√©nementielles. Nous avons contribu√© √† ce champ en optimisant le code source existant, en entra√Ænant le mod√®le am√©lior√© avec la base de donn√©es CKPLUS, et en effectuant des tests et √©valuations pour en mesurer les performances.

## Exigence de Configuration pour l'Environnement de D√©veloppement

Afin d'assurer la meilleure compatibilit√© et performance pour l'entra√Ænement de notre mod√®le Spiking-Fer et pour toute manipulation des donn√©es associ√©es, il est imp√©ratif d'utiliser **Python 3.8.10**.

## üëæ Technologies Utilis√©es

- **Python 3.8.10**
- **PyTorch et PyTorch Lightning**
- **h5py** pour la manipulation des fichiers H5
- **Syst√®me de cam√©ras √©v√©nementielles**

## üß© Installation et Configuration

1. **Clonez le d√©p√¥t** :
```
git clone https://github.com/Boubker10/Projet_P6_Reconnaissance_des_Expressions_Faciales_avec_SNN
```
3. **Installez les d√©pendances n√©cessaires** :
 ```
 pip install -r requirements.txt
 ```
5. **Assurez-vous que votre environnement de travail dispose d'un GPU compatible CUDA** pour l'entra√Ænement du mod√®le.

##  üóÇÔ∏è Structure du Projet

Le projet est organis√© de mani√®re √† faciliter l'acc√®s et la manipulation des diff√©rents √©l√©ments n√©cessaires pour l'entra√Ænement, l'√©valuation, et la mise en ≈ìuvre des mod√®les de reconnaissance des expressions faciales utilisant les r√©seaux de neurones √† impulsions (SNNs) et les cam√©ras √©v√©nementielles.

### `data/FerDVS/CKPlusDVS/`
Ce dossier contient la base de donn√©es CKPlusDVS, utilis√©e pour l'entra√Ænement et l'√©valuation du mod√®le. Ces donn√©es sont sp√©cifiquement optimis√©es pour une utilisation avec des SNNs et des cam√©ras √©v√©nementielles, offrant une richesse de d√©tails et de nuances essentielles √† la reconnaissance pr√©cise des expressions faciales.

### `experiments/`
Destin√© √† stocker les r√©sultats des diff√©rentes exp√©rimentations men√©es au cours du projet. Il inclut les logs d'entra√Ænement, les graphiques de performances, et des analyses d√©taill√©es, permettant une √©valuation rigoureuse des mod√®les d√©velopp√©s.

### `myenv/`
Contient les configurations n√©cessaires √† la cr√©ation d'un environnement virtuel Python. Ceci garantit que le projet peut √™tre reproduit avec les m√™mes d√©pendances et versions de librairies, assurant ainsi la coh√©rence et la fiabilit√© des r√©sultats.

### `project/`
Dossier racine qui encapsule les √©l√©ments cl√©s du projet, structur√© comme suit :

##### `datamodules/`
H√©berge les modules de donn√©es sp√©cifiques au projet, permettant une manipulation efficace et structur√©e des ensembles de donn√©es.

##### `fer_dvs.py`
Script destin√© √† la pr√©paration et au chargement des datasets de reconnaissance des expressions faciales (FER), utilisant des donn√©es issues de cam√©ras DVS (Dynamic Vision Sensors).

##### `models/`
R√©pertoire regroupant les diff√©rents mod√®les utilis√©s dans le projet, incluant :

- `models.py` : D√©finit les structures de base des mod√®les pour l'entra√Ænement et l'√©valuation.
- `sew_resnet.py` : Impl√©mente une variante du mod√®le ResNet, adapt√©e aux sp√©cificit√©s des SNNs.
- `snn_models.py` : Contient les d√©finitions des mod√®les de r√©seaux de neurones √† impulsions sp√©cifiquement con√ßus pour la reconnaissance des expressions faciales.

##### `utils/`
Dossier qui comprend divers utilitaires et transformations n√©cessaires au traitement des donn√©es et au fonctionnement optimal du mod√®le, incluant :

- `drop_event.py` : Utilitaire simulant la perte d'√©v√©nements dans les donn√©es DVS, utilis√© comme m√©thode d'augmentation de donn√©es.
- `dvs_noises.py` : Script d'ajout de bruit sp√©cifique aux capteurs DVS dans les donn√©es, visant √† simuler des conditions r√©elles de mani√®re plus fid√®le.
- `transform_dvs.py` : Propose des fonctions de transformation des donn√©es DVS pour leur pr√©traitement avant l'entra√Ænement ou l'√©valuation des mod√®les.
- `transforms.py` : Impl√©mente des transformations d'image g√©n√©riques et des augmentations de donn√©es utilis√©es tout au long du processus d'entra√Ænement.

### Fichiers Additionnels

- `.gitignore` : Configure Git pour ignorer les fichiers et dossiers non essentiels lors des commits.
- `LICENSE` : Document important √† consulter pour comprendre les modalit√©s d'utilisation et de partage du projet.
- `README.md` : Fournit une vue d'ensemble du projet, incluant des instructions d'installation, d'utilisation et des conseils pour contribuer.
- `output_video.mp4` : Vid√©o d√©monstrative des r√©sultats du mod√®le ou utilis√©e comme entr√©e pour les tests.
- `report_snn_CKPlusDVS.txt` : Rapport d√©taill√© des r√©sultats, analyses et conclusions de l'entra√Ænement et de l'√©valuation du mod√®le sur la base de donn√©es CKPlusDVS.
- `requirements.txt` : Liste les d√©pendances Python requises pour ex√©cuter le projet. Utilisez `pip install -r requirements.txt` pour installer ces d√©pendances dans votre environnement de travail.

## üöÄ Modifications Apport√©es au Code de `train.py`

### Nombre de Workers pour le DataLoader

- **Premier Code**: Calcule dynamiquement `train_workers` et `val_workers` en fonction de la taille du jeu de donn√©es et de la taille du lot (`batch_size`), utilisant `math.ceil(len(dataset) / batch_size)`.
- **Second Code**: Utilise une valeur fixe de 8 pour `train_workers` et `val_workers`.

### Options de DataLoader

- **Premier Code**: Ne sp√©cifie pas d'options suppl√©mentaires pour `DataLoader`.
- **Second Code**: Ajoute `persistent_workers=True` pour les `DataLoader` de formation et de validation, am√©liorant ainsi la gestion des donn√©es et l'efficacit√© du chargement.

### Configuration du Trainer de PyTorch Lightning

- **Premier Code**: Utilise `torch.cuda.device_count()` pour d√©finir le nombre de GPUs disponibles.
- **Second Code**: Emploie une condition pour utiliser 1 GPU si disponible (`1 if torch.cuda.is_available() else None`). Cette approche permet une plus grande flexibilit√© et une meilleure adaptation aux ressources mat√©rielles disponibles.

#### Ajouts Sp√©cifiques dans le Second Code

- Introduit une variable globale `xy` et un bloc de code qui capture les donn√©es d'entr√©e et les √©tiquettes du premier lot dans `train_loader`, les stocke dans `xy`, puis sort de la boucle. Cette technique est utile pour le d√©bogage et l'inspection des donn√©es.
- Ajoute `log_every_n_steps=5` dans la configuration du `Trainer`, facilitant un suivi plus fr√©quent et plus d√©taill√© de l'entra√Ænement.

Ces diff√©rences soulignent une adaptation du code pour des tests ou des d√©monstrations rapides, une am√©lioration des performances avec `persistent_workers=True`, et l'int√©gration d'une logique additionnelle pour la capture pr√©liminaire des donn√©es.

## üì¶ Utilisation de v2e pour la Conversion Vid√©o-en-√âv√©nements

#### Utilisation de v2e dans Google Colab
(https://colab.research.google.com/drive/1czx-GJnx-UkhFVBbfoACLVZs8cYlcr_M?usp=sharing).
Google Colab repr√©sente une solution pratique pour les utilisateurs cherchant √† √©viter l'installation locale de v2e ou ceux travaillant sur des syst√®mes sans les privil√®ges n√©cessaires pour les installations logicielles. Offrant un environnement de notebook Jupyter h√©berg√© dans le cloud avec acc√®s √† des ressources de calcul gratuites, Google Colab simplifie les conversions vid√©o-en-√©v√©nements. Pour utiliser v2e dans Google Colab, il suffit d'acc√©der au notebook v2e disponible sur la plateforme.

#### Installation Locale

Pour une int√©gration plus profonde de v2e dans des pipelines de donn√©es personnalis√©s, une installation locale peut √™tre pr√©f√©rable. Voici les √©tapes recommand√©es :

1. **Cr√©ation d'un Environnement Conda**: Isoler les d√©pendances de v2e pour √©viter les conflits avec d'autres installations.
2. **Installation de PyTorch et d'Autres Paquets via Conda**: Assurer la compatibilit√© et la stabilit√© de l'environnement en installant PyTorch et d'autres paquets n√©cessaires.
3. **Installation des Paquets Restants et de v2e avec pip**: Compl√©ter l'installation en ajoutant les paquets non disponibles via Conda, y compris v2e.

Suivre cet ordre d'installation minimise les probl√®mes de compatibilit√© et pr√©pare un environnement solide pour l'ex√©cution de v2e et la conversion des vid√©os en formats d'√©v√©nements adapt√©s aux SNNs.
#### D√©mo : 

-  video original 

https://github.com/Boubker10/demo/assets/116897761/bfb852bd-8f0c-4f6e-b667-ea6fbcd8d3f1  

- video event

https://github.com/Boubker10/demo/assets/116897761/bfdcd93e-7c3f-44dc-86ce-545f6e81e294
 








## ü§ñ Comment Utiliser 
### √âtape 1 : Clonage du Projet
La premi√®re √©tape consiste √† obtenir une copie locale du projet. Ceci est r√©alis√© en clonant le d√©p√¥t GitHub. Le clonage est une op√©ration cruciale car elle vous permet d'acc√©der √† toutes les ressources du projet, y compris les scripts d'entra√Ænement, les mod√®les, les utilitaires et les exigences. Utilisez la commande suivante pour cloner le projet :
```
[git clone https://github.com/Boubker10/Projet_P6_Reconnaissance_des_Expressions_Faciales_avec_SNN
```


### √âtape 2 : Installation des D√©pendances
Une fois le projet clon√©, la prochaine √©tape cruciale est l'installation des d√©pendances. Ce projet, comme beaucoup dans le domaine de l'intelligence artificielle et du machine learning, d√©pend de plusieurs biblioth√®ques externes Python. L'installation de ces d√©pendances est simplifi√©e gr√¢ce au fichier requirements.txt fourni dans le projet. En ex√©cutant la commande suivante dans le r√©pertoire du projet, toutes les d√©pendances n√©cessaires seront install√©es :
```
pip install -r requirements.txt
```

Cette √©tape assure que votre environnement Python dispose de tous les outils n√©cessaires pour l'entra√Ænement et l'√©valuation des mod√®les.


### √âtape 3 : Pr√©paration des Datasets
Avant de pouvoir lancer une exp√©rience d'entra√Ænement, il est essentiel de pr√©parer les datasets. Les donn√©es doivent √™tre plac√©es dans le dossier data/ du projet. Cette organisation facilite l'acc√®s aux donn√©es par les scripts d'entra√Ænement et permet une gestion coh√©rente des datasets. La pr√©paration des donn√©es peut inclure la conversion de vid√©os en donn√©es d'√©v√©nements si vous travaillez avec des cam√©ras DVS, un processus pour lequel des instructions d√©taill√©es sont fournies dans le projet.




### √âtape 4 : Lancement d'une Exp√©rience d'Entra√Ænement
Avec les d√©pendances install√©es et les donn√©es pr√©par√©es, vous √™tes maintenant pr√™t √† lancer une exp√©rience d'entra√Ænement. Cette √©tape est cruciale car elle permet d'entra√Æner le mod√®le sur le dataset sp√©cifi√©, en utilisant des configurations d√©finies pour les augmentations de donn√©es. Ex√©cutez la commande suivante pour d√©marrer l'entra√Ænement :

```
python train.py --dataset="CKPlusDVS" --mode="snn" --fold_number=0 --edas="flip,background_activity,crop,reverse,mirror,event_drop"

```

Cette commande configure l'entra√Ænement pour le dataset CKPlusDVS en utilisant un r√©seau de neurones √† impulsions (SNN). Elle sp√©cifie √©galement une s√©rie de transformations d'augmentation des donn√©es pour am√©liorer la robustesse et la performance du mod√®le. Les r√©sultats de cet entra√Ænement, y compris le mod√®le le mieux performant, seront sauvegard√©s dans le dossier experiments/, vous permettant d'√©valuer l'efficacit√© du mod√®le form√©.


## üß™√âvaluation et Test 

### Configuration de l'√©valuation
Pour √©valuer notre mod√®le, nous utilisons un ensemble de transformations d√©finies pour les donn√©es d'entr√©e, via la classe `DVSTransform`. Cette classe applique une s√©rie de transformations sp√©cifiques adapt√©es √† nos donn√©es, incluant :

- flip : retournement des images
- background_activity : modification de l'activit√© de fond
- crop : d√©coupe des images
- reverse : inversion des s√©quences
- mirror : effet miroir
- event_drop : suppression d'√©v√©nements

Ces transformations sont adapt√©es √† la taille du capteur des donn√©es `FerDVS` et configur√©es pour concat√©ner les canaux temporels dans un format `snn`, adapt√© pour les r√©seaux de neurones spiking.

### Chargement du mod√®le
Nous rechargeons le mod√®le √† partir d'un checkpoint sauvegard√© (`checkpoint_path`), apr√®s un entra√Ænement pr√©alable sur des donn√©es sp√©cifiques. Le mod√®le utilise une architecture de r√©seau de neurones spiking (SNN), avec plusieurs couches de convolution et des noeuds de type Integrate-and-Fire, simulant le comportement de neurones biologiques.

### Pr√©paration des donn√©es pour l'√©valuation
Pour tester le mod√®le, nous chargeons un exemple de donn√©es sous forme d'√©v√©nements depuis un fichier `.h5`. Ces √©v√©nements sont ensuite filtr√©s et transform√©s pour correspondre aux entr√©es attendues par notre mod√®le. Les donn√©es transform√©es sont converties en tenseurs PyTorch, adapt√©es √† la dimension attendue par le mod√®le avant leur soumission pour √©valuation.

### Test du mod√®le
Le mod√®le est mis en mode √©valuation (`model.eval()`), avec le calcul du gradient d√©sactiv√© pour optimiser les performances pendant les tests. Les donn√©es pr√©par√©es sont soumises au mod√®le pour obtenir des pr√©dictions. Ces pr√©dictions sont ensuite trait√©es avec une fonction softmax pour convertir les logits en probabilit√©s. La classe avec la probabilit√© la plus √©lev√©e est s√©lectionn√©e comme pr√©diction finale.

[https://github.com/Boubker10/Projet_P6_Reconnaissance_des_Expressions_Faciales_avec_SNN/assets/116897761/4a080c47-b543-417f-ad92-eead69955210](https://github.com/Boubker10/Projet_P6_Reconnaissance_des_Expressions_Faciales_avec_SNN/issues/1#issue-2241451913)


























##  üßë‚Äçüíª Contributing 
```
@title={Spiking-Fer: Spiking Neural Network for Facial Expression Recognition With Event Cameras},
  author={Boubker BENNANI , Othmane BENZARHOUNI},
  year={2024}
}
